{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248a7a2-429c-4a82-a16f-bf9b8313dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "from gym import spaces\n",
    "\n",
    "# Updated Environment: 4-joint robotic arm reaching a fixed object with improved reward and observation\n",
    "class FourJointArmEnv(gym.Env):\n",
    "    def __init__(self, render=False):\n",
    "        super(FourJointArmEnv, self).__init__()\n",
    "        self.render_mode = render\n",
    "        self.max_steps = 800\n",
    "        self.action_space = spaces.Discrete(8)  # 4 joints * 2 directions\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32)\n",
    "\n",
    "        self.physicsClient = p.connect(p.GUI if render else p.DIRECT)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        self.arm = None\n",
    "        self.target_uid = None\n",
    "        self.step_counter = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        p.loadURDF(\"plane.urdf\")\n",
    "        self.arm = p.loadURDF(\"kuka_iiwa/model.urdf\", useFixedBase=True)\n",
    "\n",
    "        x = random.uniform(0.3, 0.7)\n",
    "        y = random.uniform(-0.2, 0.2)\n",
    "        z = 0\n",
    "        self.target_pos = [x, y, z]\n",
    "        self.target_uid = p.loadURDF(\"sphere2.urdf\", self.target_pos, globalScaling=0.05)\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.prev_distance = None\n",
    "        return self._get_state(), {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        joint_angles = [p.getJointState(self.arm, i)[0] for i in range(4)]\n",
    "        return np.array(joint_angles + self.target_pos, dtype=np.float32)\n",
    "\n",
    "    def _apply_action(self, action):\n",
    "        joint = action // 2\n",
    "        direction = 0.05 if action % 2 == 0 else -0.05\n",
    "        current_angle = p.getJointState(self.arm, joint)[0]\n",
    "        new_angle = current_angle + direction\n",
    "        p.setJointMotorControl2(self.arm, joint, p.POSITION_CONTROL, targetPosition=new_angle)\n",
    "\n",
    "    def _compute_reward(self):\n",
    "        end_effector_pos = p.getLinkState(self.arm, 6)[0]\n",
    "        distance = np.linalg.norm(np.array(end_effector_pos) - np.array(self.target_pos))\n",
    "        reward = -distance\n",
    "\n",
    "        if self.prev_distance is not None:\n",
    "            reward += 10 * (self.prev_distance - distance)\n",
    "        self.prev_distance = distance\n",
    "\n",
    "        done = distance < 0.05\n",
    "        if done:\n",
    "            reward += 10\n",
    "            print(\"Touch!!!!!!!!!!!!!\")\n",
    "        return reward, done\n",
    "\n",
    "    def step(self, action):\n",
    "        self._apply_action(action)\n",
    "        for _ in range(10):\n",
    "            p.stepSimulation()\n",
    "            if self.render_mode:\n",
    "                time.sleep(1. / 240.)\n",
    "\n",
    "        self.step_counter += 1\n",
    "        obs = self._get_state()\n",
    "        reward, terminated = self._compute_reward()\n",
    "        truncated = self.step_counter >= self.max_steps\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()\n",
    "\n",
    "# DQN Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.986):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.model = DQN(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q = self.model(next_states).max(1)[0]\n",
    "        expected_q = rewards + self.gamma * next_q * (~dones)\n",
    "\n",
    "        loss = self.loss_fn(q_values, expected_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Training function\n",
    "def train_dqn(render=False, episodes=500):\n",
    "    env = FourJointArmEnv(render=render)\n",
    "    agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay epsilon once per episode\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep + 1}: Reward = {total_reward:.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
    "\n",
    "    torch.save(agent.model.state_dict(), \"dqn_4joint_arm_model.pth\")\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# Test function\n",
    "def test_trained_agent(model_path=\"dqn_4joint_arm_model.pth\", render=True):\n",
    "    env = FourJointArmEnv(render=render)\n",
    "    model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = model(state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"\\n[TEST] Total reward: {total_reward:.2f}\")\n",
    "    env.close()\n",
    "\n",
    "# Run training\n",
    "#rewards = train_dqn(render=False, episodes=80)\n",
    "\n",
    "# Visual test\n",
    "test_trained_agent(render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b962e14e-5058-41a1-bca0-b96977309064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917741ce-b4e3-49d8-bd5a-23106e5cc5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
